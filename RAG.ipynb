{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c622ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/notaehyeong/miniforge3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3511b",
   "metadata": {},
   "source": [
    "# Basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4d531",
   "metadata": {},
   "source": [
    "## Prepare documents and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67b61ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0490,  0.0476,  0.0423,  ...,  0.0570,  0.0271, -0.0106],\n",
       "        [-0.0521,  0.0510, -0.0635,  ...,  0.0877,  0.0559,  0.0605],\n",
       "        [-0.0994,  0.1269, -0.1022,  ...,  0.0842,  0.1130,  0.0064],\n",
       "        [-0.0208,  0.0361, -0.0472,  ...,  0.0449, -0.0228,  0.0603]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 문서 데이터베이스 준비\n",
    "documents = [\n",
    "    \"RAG combines retrieval and generation to enhance text generation tasks.\",\n",
    "    \"It uses a retriever to fetch relevant documents for a given query.\",\n",
    "    \"The generator then produces the final output based on the retrieved documents.\",\n",
    "    \"This approach is especially useful for knowledge-intensive tasks.\"\n",
    "]\n",
    "\n",
    "# 2. Sentence-BERT로 문서 임베딩\n",
    "retriever = SentenceTransformer('all-MiniLM-L6-v2') # change to openai embedding model\n",
    "document_embeddings = retriever.encode(documents, convert_to_tensor=True)\n",
    "document_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ca7f2",
   "metadata": {},
   "source": [
    "## Make Faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f1d7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x105863e40> >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. FAISS로 검색 인덱스 생성\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(document_embeddings.cpu()))\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9241249",
   "metadata": {},
   "source": [
    "## Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cd70b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.71513635e-02,  7.33634606e-02,  3.93800028e-02,  1.04654785e-02,\n",
       "       -6.15194142e-02,  5.47023350e-03,  5.94053306e-02,  6.10240139e-02,\n",
       "       -1.78641174e-02, -3.20010968e-02, -9.42157023e-03,  4.57748771e-02,\n",
       "        2.68447082e-02, -4.86106351e-02, -7.31136352e-02,  8.20814893e-02,\n",
       "        6.39288574e-02,  1.02431938e-01, -1.98324285e-02, -3.61080207e-02,\n",
       "       -1.20374588e-02,  3.08231898e-02, -2.75140628e-02, -4.36160602e-02,\n",
       "        1.96681432e-02,  5.38504906e-02, -4.66142744e-02,  5.46863908e-03,\n",
       "        6.40840530e-02, -1.74713172e-02,  9.10303928e-03,  4.85722125e-02,\n",
       "       -3.18480544e-02, -2.24165376e-02, -6.25128746e-02,  1.96867865e-02,\n",
       "       -2.03983709e-02,  5.88036850e-02, -1.53918294e-02,  3.69462259e-02,\n",
       "        3.92769389e-02, -3.10819726e-02, -2.80910823e-02, -7.99766928e-02,\n",
       "        2.43709255e-02,  3.64410095e-02, -2.06157775e-03,  4.76543419e-02,\n",
       "        1.13168210e-02, -2.39392892e-02, -8.32374115e-03,  4.24382696e-03,\n",
       "        4.91901115e-02,  6.17353767e-02, -4.15207930e-02, -8.22640210e-02,\n",
       "        4.94626537e-02, -5.72495535e-02, -3.95661183e-02,  1.88762117e-02,\n",
       "       -4.07238528e-02, -2.57746316e-02, -1.11501326e-03,  3.89420763e-02,\n",
       "        1.45804957e-01, -1.08508781e-01,  2.53243819e-02,  4.62534539e-02,\n",
       "       -2.88155512e-03, -2.81306338e-02, -9.20731500e-02, -2.44478695e-02,\n",
       "       -3.23677585e-02,  1.03544079e-01,  3.80460024e-02,  2.15381803e-03,\n",
       "       -1.34037938e-02, -1.28293484e-02, -2.11533438e-02, -4.14791182e-02,\n",
       "       -7.81515688e-02,  1.93890110e-02,  2.16458850e-02,  5.50737418e-02,\n",
       "        1.65327862e-02,  8.79902318e-02, -1.75109338e-02,  6.91362564e-03,\n",
       "        1.96891837e-02, -3.46596837e-02,  2.87326630e-02,  3.44795212e-02,\n",
       "       -3.31471977e-03, -5.07724695e-02,  3.56347337e-02,  3.44446748e-02,\n",
       "        2.85980143e-02,  1.10468768e-01, -2.01419052e-02,  3.11958212e-02,\n",
       "       -4.82918955e-02, -6.15283288e-03,  1.64155010e-02, -3.30833793e-02,\n",
       "       -4.34498601e-02, -8.75761509e-02, -5.28966310e-04, -5.83230369e-02,\n",
       "       -3.31938230e-02, -9.87313688e-03,  3.35418396e-02, -1.97285209e-02,\n",
       "       -3.00771017e-02, -4.25265618e-02, -1.28938869e-01,  1.89321991e-02,\n",
       "        8.75864178e-02, -2.18965821e-02, -1.15430392e-02,  4.38754149e-02,\n",
       "        3.51849832e-02,  3.12200859e-02, -7.74136782e-02,  2.22287383e-02,\n",
       "       -1.16501749e-02,  6.99192192e-03,  7.99451396e-02, -9.18295893e-33,\n",
       "        2.40097381e-03, -5.91144003e-02, -2.01279856e-03,  1.13802934e-02,\n",
       "       -6.70074299e-02, -3.73264439e-02, -5.19585758e-02, -8.35432485e-02,\n",
       "        7.09761605e-02,  7.48567358e-02,  5.38975932e-02,  6.14074816e-04,\n",
       "        2.81518530e-02,  7.19705969e-02,  3.33272503e-03,  3.59780937e-02,\n",
       "       -3.41411121e-02, -6.54281527e-02,  5.34578189e-02,  2.17602737e-02,\n",
       "       -4.12280038e-02,  1.23364806e-01, -3.16242059e-03,  5.68486005e-03,\n",
       "       -1.24693505e-01,  9.11155418e-02,  9.15297214e-03, -1.65262278e-02,\n",
       "       -4.79759015e-02,  2.54386035e-03,  1.10221496e-02,  3.17520648e-02,\n",
       "       -4.87074926e-02,  1.87956821e-02, -1.52113494e-02, -4.14642319e-03,\n",
       "       -9.80186313e-02, -3.02231498e-02, -5.60263731e-02,  3.67259444e-03,\n",
       "       -3.67050171e-02, -3.55439000e-02, -6.95900023e-02, -5.44012152e-02,\n",
       "        7.08695278e-02, -6.84052259e-02,  9.19885468e-03,  6.17474467e-02,\n",
       "        8.58760439e-03,  1.23237535e-01,  1.12324432e-01,  2.08116006e-02,\n",
       "        2.58782767e-02,  3.59437019e-02, -4.18891124e-02,  8.66451263e-02,\n",
       "        5.89776859e-02, -1.07558131e-01, -6.29049540e-02,  1.18233480e-01,\n",
       "       -1.50662614e-02,  3.36211175e-02,  4.60955751e-04,  6.36396781e-02,\n",
       "        9.71276953e-04, -3.06788478e-02,  2.03029215e-02,  2.62489486e-02,\n",
       "        3.88629735e-02, -2.85081025e-02,  4.30258317e-03, -1.42958034e-02,\n",
       "        9.22939181e-03,  7.69115835e-02, -3.80473696e-02, -8.08073580e-03,\n",
       "        3.96276787e-02,  3.92689779e-02,  3.15794758e-02, -3.48513126e-02,\n",
       "        1.81553736e-02, -1.78259816e-02, -2.05634050e-02, -4.10721153e-02,\n",
       "       -7.69583359e-02,  2.52954345e-02, -3.38405259e-02, -7.07123131e-02,\n",
       "       -4.65217754e-02, -8.77223760e-02, -2.47985707e-03, -4.39320840e-02,\n",
       "       -1.93959959e-02,  3.28203440e-02,  6.50301669e-03,  2.93612683e-33,\n",
       "        1.00750744e-01, -2.74285488e-02,  3.11055547e-03,  5.77873290e-02,\n",
       "        1.79259684e-02,  2.94643827e-03, -3.16356905e-02,  5.05045801e-02,\n",
       "       -4.43323329e-02,  1.01036653e-01,  1.80294854e-03, -6.62555313e-03,\n",
       "       -1.36958109e-02,  2.55818013e-02,  7.71337375e-02,  8.05019587e-02,\n",
       "        1.95680447e-02,  4.90449294e-02, -6.76039085e-02,  3.05794682e-02,\n",
       "        3.06707788e-02,  6.33434653e-02,  7.71390051e-02,  1.76656414e-02,\n",
       "       -6.03785105e-02, -7.21971020e-02,  6.43468350e-02, -7.06498176e-02,\n",
       "       -1.58178080e-02, -1.04761263e-02,  9.52990074e-03, -8.05334672e-02,\n",
       "       -1.48338536e-02, -5.03779612e-02,  1.16042336e-02, -6.88279793e-02,\n",
       "        7.71260783e-02,  3.74594480e-02, -5.79570718e-02, -1.53148755e-01,\n",
       "        1.30543206e-02, -2.72824373e-02,  1.64780393e-02, -1.70447975e-02,\n",
       "       -2.77058389e-02,  6.42608032e-02, -9.77259651e-02, -8.70838482e-03,\n",
       "       -9.03457552e-02, -9.93153825e-03,  8.07373077e-02,  1.14555983e-02,\n",
       "        7.51424581e-02, -4.96692248e-02, -9.90131870e-03, -2.32788501e-03,\n",
       "       -2.79605370e-02, -4.80041020e-02, -4.16775160e-02,  1.33118913e-01,\n",
       "        2.80882623e-02,  4.58296649e-02, -2.56373622e-02,  2.81249620e-02,\n",
       "        4.90913261e-03,  2.52319816e-02, -5.86327426e-02, -5.94021119e-02,\n",
       "       -1.22269042e-01,  5.12230536e-03,  3.21001001e-02,  2.62005720e-02,\n",
       "        4.23171222e-02, -6.11822233e-02, -3.14383693e-02, -8.25000741e-03,\n",
       "        5.08043468e-02, -1.21525139e-01, -2.35580578e-02, -3.47368531e-02,\n",
       "       -5.94380833e-02, -1.07301936e-01,  3.48363072e-02,  3.12033556e-02,\n",
       "       -7.01338872e-02, -4.75922264e-02, -1.50880367e-01,  2.83506569e-02,\n",
       "        3.95078026e-02, -3.08918897e-02,  1.87621601e-02,  3.05577349e-02,\n",
       "       -7.26352185e-02, -1.77291909e-03,  4.35074568e-02, -1.73885404e-08,\n",
       "        4.81331125e-02, -6.34487048e-02, -2.86154798e-03, -4.37962674e-02,\n",
       "       -1.00978628e-01, -1.59870014e-02, -2.01132540e-02,  7.46599212e-03,\n",
       "        3.71387005e-02,  8.69011283e-02, -7.05071632e-03,  1.71517860e-02,\n",
       "       -2.23180670e-02, -1.42867845e-02,  4.20986935e-02,  1.58179794e-02,\n",
       "       -3.51610919e-03, -1.72521418e-03, -7.82280043e-02, -7.68521130e-02,\n",
       "       -2.36805473e-02, -2.04650741e-02,  7.25402460e-02,  2.56244652e-02,\n",
       "       -8.26535630e-04, -2.21327860e-02,  3.30887586e-02,  1.08036406e-01,\n",
       "       -1.27010362e-03, -4.00823988e-02,  5.47321290e-02,  4.79747541e-02,\n",
       "       -6.53755153e-04, -6.89724982e-02, -3.28296237e-02,  1.25276316e-02,\n",
       "        6.11621998e-02, -9.26460624e-02,  3.97761315e-02,  3.53845395e-02,\n",
       "       -3.22830267e-02,  4.23215851e-02,  1.89775098e-02, -5.30053973e-02,\n",
       "        3.77581380e-02,  4.71384358e-03, -7.94036388e-02, -9.17757489e-03,\n",
       "       -5.55560067e-02, -1.04414197e-02,  2.27261558e-02,  1.39721259e-02,\n",
       "        3.87272751e-03,  4.45411727e-02, -7.38394558e-02, -1.77134387e-02,\n",
       "        4.74263839e-02, -6.73022121e-02,  5.55362254e-02,  1.61370654e-02,\n",
       "        5.18951677e-02,  2.24594586e-02,  8.49379972e-02,  5.59766628e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 질문 정의\n",
    "query = \"What is RAG and how does it work?\"\n",
    "query_embedding = retriever.encode(query, convert_to_tensor=True).cpu().numpy()\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7499773",
   "metadata": {},
   "source": [
    "## Retrieval documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfdf920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0134497, 1.6782237]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 가장 유사한 문서 검색\n",
    "k = 2  # Top-k 검색\n",
    "distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dfa24",
   "metadata": {},
   "source": [
    "## See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70f140be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAG combines retrieval and generation to enhance text generation tasks.',\n",
       " 'It uses a retriever to fetch relevant documents for a given query.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색된 문서 가져오기\n",
    "retrieved_docs = [documents[i] for i in indices[0]]\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876f4b5",
   "metadata": {},
   "source": [
    "# Webpage RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfdca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13141\n",
      " 위해 좀 더 빠르게 강력한 수단을 이용해야 합니다. 특히 정책 문서에 명시된 원칙을 지키지 않는 것은 대부분의 경우 다른 사용자에게 받아들여지지 않습니다 (다른 분들에게 예외 상황임을 설득할 수 있다면 가능하기는 하지만요). 이는 당신을 포함해서 편집자 개개인이 정책과 지침을 직접 집행 및 적용한다는 것을 의미합니다.\n",
      "특정 사용자가 명백히 정책에 반하는 행동을 하거나 정책과 상충되는 방식으로 지침을 어기는 경우, 특히 의도적이고 지속적으로 그런 행위를 하는 경우 해당 사용자는 관리자의 제재 조치로 일시적, 혹은 영구적으로 편집이 차단될 수 있습니다. 영어판을 비롯한 타 언어판에서는 일반적인 분쟁 해결 절차로 끝낼 수 없는 사안은 중재위원회가 개입하기도 합니다.\n",
      "\n",
      "문서 내용\n",
      "정책과 지침의 문서 내용은 처음 읽는 사용자라도 원칙과 규범을 잘 이해할 수 있도록 다음 원칙을 지켜야 합니다.\n",
      "\n",
      "명확하게 작성하세요. 소수만 알아듣거나 준법률적인 단어, 혹은 지나치게 단순한 표현은 피해야 합니다. 명확하고, 직접적이고, 모호하지 않고, 구체적으로 작성하세요. 지나치게 상투적인 표현이나 일반론은 피하세요. 지침, 도움말 문서 및 기타 정보문 문서에서도 \"해야 합니다\" 혹은 \"하지 말아야 합니다\" 같이 직접적인 표현을 굳이 꺼릴 필요는 없습니다.\n",
      "가능한 간결하게, 너무 단순하지는 않게. 정책이 중언부언하면 오해를 부릅니다. 불필요한 말은 생략하세요. 직접적이고 간결한 설명이 마구잡이식 예시 나열보다 더 이해하기 쉽습니다. 각주나 관련 문서 링크를 이용하여 더 상세히 설명할 수도 있습니다.\n",
      "규칙을 만든 의도를 강조하세요. 사용자들이 상식대로 행동하리라 기대하세요. 정책의 의도가 명료하다면, 추가 설명은 필요 없죠. 즉 규칙을 '어떻게' 지키는지와 더불어 '왜' 지켜야 하는지 확실하게 밝혀야 합니다.\n",
      "범위는 분명히, 중복은 피하기. 되도록 앞부분에서 정책 및 지침의 목적과 범위를 분명하게 밝혀야 합니다. 독자 대부분은 도입부 초반만 읽고 나가버리니까요. 각 정책 문서의 내용은 \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader # Get webpage\n",
    "\n",
    "url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "docs = loader.load()# text-> Documents\n",
    "\n",
    "print(len(docs))\n",
    "print(len(docs[0].page_content))\n",
    "print(docs[0].page_content[5000:6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c9fba",
   "metadata": {},
   "source": [
    "## Split documents to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846ccce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "page_content='채택 과정\\n한국어 위키백과에서 오랫동안 확립되어 온 정책과 지침의 대다수는, 영어 위키백과 설립 시 토대가 된 원칙에서 발전된 것들입니다. 물론 타 언어 위키백과의 원칙을 가져오는 것 말고도, 정책과 지침을 일반적인 문제와 문서 훼손 행위의 대응책으로 한국어 위키백과 내 공동체에서 자발적으로 세우기도 했습니다. 정책과 지침 대부분은 전례 없이 바로 받아들여지기보다, 공동체의 강력한 지지를 바탕으로 세워집니다. 정책과 지침의 제정 방법으로는 제안을 통한 수립, 기존의 수필 또는 지침의 정책화, 기존의 정책과 지침의 분할 또는 합병을 통한 재구성 등의 여러 방법이 있습니다.\\n정책과 지침이 아닌 위키백과 내 운영과 관련된 문서에는 {{수필}}, {{정보문}}, {{위키백과 사용서}} 등을 붙여 구분해야 합니다.\\n현재 정책이나 지침으로 제안된 문서는 분류:위키백과 제안에 모여 있습니다. 총의를 통해 채택이 거부된 제안은 분류:위키백과 거부된 제안을 참조하세요. 여러분들의 참여를 환영합니다.' metadata={'source': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8', 'title': '위키백과:정책과 지침 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}\n"
     ]
    }
   ],
   "source": [
    "# Text Split (Documents -> small chunks: Documents)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(splits))\n",
    "print(splits[9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78eb44",
   "metadata": {},
   "source": [
    "## Indexing using chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "292b81e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Indexing (Texts -> Embedding -> Store)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings(openai_api_key=API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "294dca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "채택 과정\n",
      "한국어 위키백과에서 오랫동안 확립되어 온 정책과 지침의 대다수는, 영어 위키백과 설립 시 토대가 된 원칙에서 발전된 것들입니다. 물론 타 언어 위키백과의 원칙을 가져오는 것 말고도, 정책과 지침을 일반적인 문제와 문서 훼손 행위의 대응책으로 한국어 위키백과 내 공동체에서 자발적으로 세우기도 했습니다. 정책과 지침 대부분은 전례 없이 바로 받아들여지기보다, 공동체의 강력한 지지를 바탕으로 세워집니다. 정책과 지침의 제정 방법으로는 제안을 통한 수립, 기존의 수필 또는 지침의 정책화, 기존의 정책과 지침의 분할 또는 합병을 통한 재구성 등의 여러 방법이 있습니다.\n",
      "정책과 지침이 아닌 위키백과 내 운영과 관련된 문서에는 {{수필}}, {{정보문}}, {{위키백과 사용서}} 등을 붙여 구분해야 합니다.\n",
      "현재 정책이나 지침으로 제안된 문서는 분류:위키백과 제안에 모여 있습니다. 총의를 통해 채택이 거부된 제안은 분류:위키백과 거부된 제안을 참조하세요. 여러분들의 참여를 환영합니다.\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(\"채택 과정 대해서 설명해주세요.\")\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992e1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LangChain을 구성하기 위한 필수 모듈들 import\n",
    "# - ChatOpenAI: OpenAI의 GPT 모델과 상호작용\n",
    "# - ChatPromptTemplate: 프롬프트 템플릿 정의\n",
    "# - RunnablePassthrough: 입력을 그대로 전달하는 도구\n",
    "# - StrOutputParser: LLM 출력 문자열을 파싱하는 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81e92fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Please provide most correct answer from the following context. \\nIf the answer is not present in the context, please write \"The information is not present in the context.\"\\n---\\nQuestion: {question}\\n---\\nContext: {context}\\n'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = '''Please provide most correct answer from the following context. \n",
    "If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "---\n",
    "Question: {question}\n",
    "---\n",
    "Context: {context}\n",
    "---\n",
    "Output format:\n",
    "gradient: ...\n",
    "Badges: Gluten o\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "# 질문과 컨텍스트를 기반으로 한 LLM의 입력 프롬프트를 정의\n",
    "# {question}와 {context}는 이후에 실제 데이터로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e294dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.split('Badesg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42686824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0, openai_api_key=API_KEY)\n",
    "# OpenAI의 GPT-4o-mini 모델을 사용할 LLM 객체 생성\n",
    "# - 모델 이름: 'gpt-4o-mini|'\n",
    "# - temperature=0: 출력의 일관성을 높이기 위해 탐색성을 최소화\n",
    "# - openai_api_key: OpenAI API 인증 키 (미리 정의된 값 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b85ac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "# 벡터 스토어(vectorstore)를 기반으로 유사한 문서를 검색하는 retriever 생성\n",
    "# vectorstore는 위에서 문서 임베딩 데이터를 저장한 chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f2fdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "# 검색된 문서들(docs)을 하나의 문자열로 결합하는 함수\n",
    "# - 각 문서의 내용을 개행(\\n\\n)으로 구분하여 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4621afbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'피해야 하는 과정에 대해서는 다음과 같은 내용이 있습니다: 소수만 알아듣거나 준법률적인 단어, 지나치게 단순한 표현은 피해야 하며, 지나치게 상투적인 표현이나 일반론도 피해야 합니다. 또한, 과도한 링크는 피하고, 서로 모순되는 내용이 있어서는 안 되며, 정책과 지침은 백과사전의 일부가 아니므로 일반적인 문서와 같은 내용 정책이나 지침을 적용할 필요가 없습니다.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Chain 연결\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    # 컨텍스트(context): retriever에서 검색된 문서를 format_docs로 처리\n",
    "    # 질문(question): 그대로 전달\n",
    "    | prompt\n",
    "    # 질문과 컨텍스트를 프롬프트 템플릿(prompt)에 연결\n",
    "    | model\n",
    "    # 프롬프트를 LLM 모델에 전달하여 결과 생성\n",
    "    | StrOutputParser()\n",
    "    # LLM의 출력을 문자열로 파싱하여 반환\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain.invoke(\"피해야하는 과정에 대해서 설명해주세요.\")\n",
    "# RAG 체인을 실행하여 질문을 처리\n",
    "# - retriever가 질문에 대한 유사 문서를 검색\n",
    "# - 검색된 문서를 프롬프트와 결합하여 모델에 전달\n",
    "# - 생성된 답변을 문자열로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e81fb",
   "metadata": {},
   "source": [
    "# PDF RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861f79a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size Type H6 (Avg.) ARC HellaSwag MMLU TruthfulQA Winogrande GSM8K\n",
      "SOLAR 10.7B-Instruct⇠11B Alignment-tuned74.20 71.08 88.16 66.21 71.43 83.58 64.75Qwen 72B ⇠72B Pretrained 73.60 65.19 85.94 77.37 60.19 82.48 70.43Mixtral 8x7B-Instruct-v0.1⇠47B Instruction-tuned 72.62 70.22 87.63 71.16 64.58 81.37 60.73Yi 34B-200K ⇠34B Pretrained 70.81 65.36 85.58 76.06 53.64 82.56 61.64Yi 34B ⇠34B Pretrained 69.42 64.59 85.69 76.35 56.23 83.03 50.64Mixtral 8x7B-v0.1⇠47B Pretrained 68.42 66.04 86.49 71.82 46.78 81.93 57.47Llama 2 70B ⇠70B Pretrained 67.87 67.32 87.33 69.83 44.92 83.74 54.06Falcon 180B ⇠180B Pretrained 67.85 69.4588.86 70.50 45.47 86.90 45.94SOLAR 10.7B ⇠11B Pretrained 66.04 61.95 84.60 65.48 45.04 83.66 55.50Qwen 14B ⇠14B Pretrained 65.86 58.28 83.99 67.70 49.43 76.80 58.98Mistral 7B-Instruct-v0.2⇠7B Instruction-tuned 65.71 63.14 84.88 60.78 68.26 77.19 40.03Yi 34B-Chat ⇠34B Instruction-tuned 65.32 65.44 84.16 74.90 55.37 80.11 31.92Mistral 7B ⇠7B Pretrained 60.97 59.98 83.31 64.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# PyPDFLoader를 import: PDF 파일을 로드하고 내용을 추출하는 데 사용되는 LangChain의 커뮤니티 문서 로더\n",
    "# PyPDFLoader는 PDF에서 텍스트를 효율적으로 추출하는 기능을 제공\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"pdfs/solar_sample.pdf\")\n",
    "# 'solar_sample.pdf' 파일 경로를 사용하여 PyPDFLoader 객체 생성\n",
    "# pdfs/ 폴더에 있는 solar_sample.pdf 파일을 로드할 준비\n",
    "# loader 객체를 통해 PDF 내용을 텍스트로 변환하고 문서(Document) 객체로 반환 가능\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "# PDF 파일을 로드하여 문서(Document) 객체 리스트를 반환\n",
    "# 각 Document 객체는 PDF의 한 페이지를 나타내며, 페이지 내용(page_content)과 메타데이터(metadata)를 포함\n",
    "# - loader.load(): PDF 파일의 모든 페이지를 한 번에 로드\n",
    "# - loader.lazy_load(): 페이지를 필요한 시점에 로드하는 지연 로딩 방식 (메모리 효율성)\n",
    "\n",
    "print(docs[0].page_content[:1000])\n",
    "# 첫 번째 페이지의 내용을 출력\n",
    "# page_content는 문서(Document) 객체의 텍스트 콘텐츠\n",
    "# [:1000]: 최대 1000자만 출력하여 내용의 일부를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e902a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['Context', 'question'], template='\\n    Please provide most correct answer from the following context. \\n    If the answer is not present in the context, please write \"The information is not present in the context.\"\\n    ---\\n    Question: {question}\\n    ---\\n    Context: {Context}\\n    ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x353b262c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x353b44070>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0, openai_api_key=API_KEY)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "# PromptTemplate 객체 생성\n",
    "# - 입력 템플릿 정의: 질문과 컨텍스트를 결합하여 프롬프트를 구성\n",
    "# - {question}: 사용자가 입력할 질문\n",
    "# - {Context}: 검색된 문서나 추가 정보가 삽입될 자리\n",
    "# - 이 템플릿은 RAG (Retrieval-Augmented Generation)에서 사용됨\n",
    "\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "# 체인(Chain) 구성: LangChain의 체인 연산자를 사용하여 여러 컴포넌트를 연결\n",
    "# 1. prompt_template: 프롬프트 생성 (질문과 컨텍스트 삽입)\n",
    "# 2. model: LLM에 프롬프트 전달 및 답변 생성\n",
    "# 3. StrOutputParser: 모델의 출력을 문자열로 변환하여 반환\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b34df",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70697af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 2 presents evaluation results from the Open LLM Leaderboard for the SOLAR 10.7B and SOLAR 10.7B-Instruct models, along with other top-performing models. It includes scores for six tasks and the H6 score, which is the average of these tasks. The table also indicates the size of the models in billions of parameters and categorizes them based on their training stage as either Pretrained, Instruction-tuned, or Alignment-tuned. Models based on SOLAR 10.7B are highlighted in purple, and the best scores for both H6 and individual tasks are shown in bold.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Explain Table 2?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d23c4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The MMLU score of SOLAR 10.7B is 84.60.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is MMLU scores of SOLAR 10.7B?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b2a5e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The information is not present in the context.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is ARC of Mistral?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32ae56",
   "metadata": {},
   "source": [
    "## Simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "983982a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇을 시작합니다. 'exit'를 입력하면 종료됩니다.\n",
      "질문: What is MMLU scores of Falcon 180B?\n",
      "답변: The MMLU score of Falcon 180B is 88.86.\n",
      "질문: What is ARC of Mistral?\n",
      "답변: The information is not present in the context.\n",
      "질문: exit\n",
      "챗봇을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "def chatbot(docs):\n",
    "    print(\"챗봇을 시작합니다. 'exit'를 입력하면 종료됩니다.\")\n",
    "    while True:\n",
    "        question = input(\"질문: \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"챗봇을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        context = docs\n",
    "        response = chain.invoke({\"question\": question, \"Context\": context})\n",
    "        print(\"답변:\", response)\n",
    "\n",
    "# 챗봇 실행\n",
    "chatbot(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9701e3",
   "metadata": {},
   "source": [
    "# Smart RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b59014",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_summary = \"\"\"\n",
    "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\n",
    "\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1caf049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DUS stands for depth up-scaling, which is a method for scaling large language models (LLMs) that includes depthwise scaling and continued pretraining.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0, openai_api_key=API_KEY)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"question\": \"What is DUS?\", \"context\": solar_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2778121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The information is not present in the context.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How to get to Seoul from SF\", \"context\": solar_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20a849e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG or Search?\n",
    "def is_in(question, context):\n",
    "    is_in_conetxt = \"\"\"As a helpful assistant, \n",
    "please use your best judgment to determine if the answer to the question is within the given context. \n",
    "If the answer is present in the context, please respond with \"yes\". \n",
    "If not, please respond with \"no\". \n",
    "Only provide \"yes\" or \"no\" and avoid including any additional information. \n",
    "Please do your best. Here is the question and the context:\n",
    "---\n",
    "CONTEXT: {context}\n",
    "---\n",
    "QUESTION: {question}\n",
    "---\n",
    "OUTPUT (yes or no):\"\"\"\n",
    "\n",
    "    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)\n",
    "    chain = is_in_prompt | model | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    print(response)\n",
    "    return response.lower().startswith(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdb17e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_in(\"How to get to Seoul from SF\", solar_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89c5837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "def smart_rag(question, context):\n",
    "    # 함수 정의: `smart_rag`는 질문과 초기 컨텍스트를 받아 RAG 기반의 답변을 반환.\n",
    "    # 매개변수:\n",
    "    # - `question`: 사용자가 입력한 질문.\n",
    "    # - `context`: 질문에 대한 기존의 컨텍스트(문서, 정보 등).\n",
    "\n",
    "    if not is_in(question, context):\n",
    "        # 조건: 질문에 대한 답변이 기존 `context`에 포함되어 있지 않은 경우.\n",
    "        # `is_in`: 사용자가 정의한 함수로 보이며, 질문에 대한 답변이 컨텍스트에 있는지 확인.\n",
    "\n",
    "        print(\"Searching in tavily\")\n",
    "        tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        # Tavily API 클라이언트를 생성.\n",
    "        # 환경 변수에서 API 키를 가져와 `TavilyClient`에 전달.\n",
    "        # - `os.environ[\"TAVILY_API_KEY\"]`: 환경 변수에 저장된 Tavily API 키.\n",
    "\n",
    "        context = tavily.search(query=question)\n",
    "        # Tavily 검색 엔진을 사용하여 질문(`question`)과 관련된 컨텍스트를 검색.\n",
    "        # `context` 변수에 검색된 문서나 정보를 저장.\n",
    "\n",
    "\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    # LangChain 체인을 생성:\n",
    "    # - `prompt_template`: 질문과 컨텍스트를 조합한 프롬프트 생성.\n",
    "    # - `model`: GPT 모델로 질문과 컨텍스트를 전달하여 답변 생성.\n",
    "    # - `StrOutputParser()`: 모델 출력 결과를 문자열로 변환.\n",
    "\n",
    "    return chain.invoke({\"context\": context, \"question\": question})\n",
    "    # 체인을 실행하여 최종 답변을 반환:\n",
    "    # - `context`: 검색된 컨텍스트.\n",
    "    # - `question`: 사용자 질문.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91007376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "Searching in tavily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To get to Seoul from San Francisco, you can fly from San Francisco International Airport (SFO) to Incheon International Airport (ICN). There are various options available, including comparing ticket prices and travel times through travel planners like Rome2Rio. For more details, you can check the following resources:\\n\\n1. [Rome2rio](https://www.rome2rio.com/s/San-Francisco/Seoul)\\n2. [Google Flights](https://www.google.com/travel/flights/flights-from-san-francisco-to-seoul.html)\\n3. [Skyscanner](https://www.skyscanner.com/routes/sfo/sela/san-francisco-international-to-seoul.html)\\n4. [KAYAK](https://www.kayak.com/flight-routes/San-Francisco-SFO/Seoul-SEL)\\n5. [Expedia](https://www.expedia.com/lp/flights/sfo/icn/san-francisco-to-seoul)\\n\\nThese resources can help you find flights and plan your trip effectively.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_rag(\"How to get to Seoul from SF?\", solar_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80e48f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "Searching in tavily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main ingredients of rose tteokbokki are gochujang, cream, soy sauce, and tteokbokki-tteok (rice cakes).'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_rag(\"What ingredient of rose tuckboki?\", solar_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef318b8",
   "metadata": {},
   "source": [
    "# ReACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5ab8ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TavilySearchResults()]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search.tool import TavilySearchResults\n",
    "# LangChain의 커뮤니티 도구에서 Tavily 검색 도구(TavilySearchResults)를 가져옴.\n",
    "# TavilySearchResults는 질문에 대해 Tavily 검색 엔진을 호출하고 관련 문서를 반환하는 도구.\n",
    "\n",
    "\n",
    "tools = [TavilySearchResults(max_results=5)]\n",
    "# TavilySearchResults 도구를 사용하여 검색 기능을 구성:\n",
    "# - max_results=5: 검색 결과의 최대 개수를 5개로 제한.\n",
    "# - TavilySearchResults 객체를 리스트로 저장. LangChain 워크플로에서 사용할 수 있도록 준비.\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94d5aab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/notaehyeong/miniforge3/envs/llm/lib/python3.10/site-packages/langchain/hub.py:83: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.tools.render import render_text_description\n",
    "# `hub`: LangChain의 허브를 사용하여 사전에 정의된 리소스(모델, 프롬프트 등)를 가져오는 모듈.\n",
    "# `render_text_description`: LangChain의 도구에 대한 텍스트 설명을 생성하는 유틸리티 함수.\n",
    "# - 도구들의 기능 및 목적을 간결하게 서술하는 데 사용.\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "# LangChain 허브에서 \"hwchase17/react-chat\" 프롬프트를 가져옴.\n",
    "# - \"hwchase17/react-chat\": ReAct(Reinforcement Learning with Action and Thought) 방식으로 설계된 사전 정의된 프롬프트.\n",
    "# - `hub.pull()`은 LangChain 허브에서 특정 리소스를 불러오는 메서드.\n",
    "# 이 프롬프트는 주로 도구와 상호작용하는 챗봇을 위한 템플릿을 포함.\n",
    "\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "# 가져온 프롬프트를 `partial()` 메서드를 사용하여 커스터마이징.\n",
    "# - `tools`: LangChain 도구에 대한 설명을 텍스트로 생성한 결과.\n",
    "# - `tool_names`: 도구들의 이름을 쉼표로 구분한 문자열 형태로 생성.\n",
    "# 결과적으로, 이 프롬프트는 특정 도구 정보(`tools`와 `tool_names`)를 포함하여 재정의됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77da04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "# LangChain의 `ConversationBufferMemory` 클래스를 import.\n",
    "# `ConversationBufferMemory`는 대화 내용을 메모리(버퍼)에 저장하여, 이전 대화 기록을 유지하고 사용할 수 있도록 지원.\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "# `ConversationBufferMemory` 객체를 생성:\n",
    "# - `memory_key`: 저장된 대화 기록을 참조할 때 사용할 키. 여기서는 \"chat_history\"로 설정.\n",
    "# - 이 메모리는 LangChain 워크플로에서 사용되며, 사용자와의 대화 기록을 관리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "909b53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0, openai_api_key=API_KEY)\n",
    "model_with_stop = model.bind(stop=[\"\\nObservation\"])\n",
    "# `model.bind()` 메서드는 기존 모델(`model`)의 일부 매개변수(파라미터)를 고정한 새로운 모델 객체를 생성.\n",
    "# - `stop`: 모델이 응답을 생성할 때 멈추는 조건(stop tokens)을 지정.\n",
    "# - `[\"\\nObservation\"]`: 모델이 텍스트를 생성하는 동안 `\"\\nObservation\"`이 등장하면 응답 생성을 중단."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb7b2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "# AgentExecutor는 LangChain에서 에이전트를 실행하는 데 사용되는 클래스.\n",
    "# - 에이전트: LLM과 도구를 결합하여 지능적인 작업을 수행하는 구성 요소.\n",
    "# AgentExecutor는 이러한 에이전트와 연관된 도구 및 메모리를 사용해 체계를 실행.\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "# `format_log_to_str`는 중간 단계의 로그(intermediate_steps)를 문자열로 변환하는 유틸리티 함수.\n",
    "# - ReAct 방식에서 모델이 생성한 중간 사고 과정(Thought)과 관찰(Observation)을 정리.\n",
    "\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "# `ReActSingleInputOutputParser`는 ReAct 방식에서 단일 입력/출력을 처리하는 파서.\n",
    "# - 입력: 질문 및 컨텍스트.\n",
    "# - 출력: 모델의 최종 응답.\n",
    "# ReAct는 모델이 \"사고(Thought)\"와 \"행동(Action)\" 단계를 거쳐 결과를 생성하는 구조를 사용.\n",
    "\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        # 사용자 입력에서 \"input\" 키를 추출하여 전달.\n",
    "        \n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        # 중간 단계(`intermediate_steps`)를 문자열로 변환하여 `agent_scratchpad`에 저장.\n",
    "        \n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        # 이전 대화 기록(`chat_history`)를 유지하여 대화형 작업에서 컨텍스트로 사용.\n",
    "    }\n",
    "    | prompt\n",
    "    # 사용자 입력(`input`), 중간 단계(`agent_scratchpad`), 대화 기록(`chat_history`)을 프롬프트와 결합.\n",
    "\n",
    "    | model_with_stop\n",
    "    # 프롬프트를 GPT 모델(`model_with_stop`)에 전달하여 응답 생성.\n",
    "\n",
    "    | ReActSingleInputOutputParser()\n",
    "    # 생성된 응답을 ReAct 스타일의 출력으로 파싱.\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "# AgentExecutor는 위에서 정의한 에이전트와 관련 도구, 메모리를 결합하여 실행 가능한 객체 생성.\n",
    "# 매개변수:\n",
    "# - `agent`: 위에서 정의한 에이전트. 입력 처리, 프롬프트 생성, 모델 호출, 출력 파싱을 포함.\n",
    "# - `tools`: 에이전트가 사용할 LangChain 도구 리스트. (예: 검색 도구, 계산 도구 등.)\n",
    "# - `memory`: 대화 메모리 객체. (이전 대화 내용을 참조하여 응답의 일관성을 유지.)\n",
    "# - `verbose=True`: 실행 중 상세 로그를 출력하여 디버깅 및 상태 확인 가능.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52a2680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "Final Answer: 세종대왕(세종대왕, 1397-1450)은 조선의 제4대 왕으로, 한글을 창제한 것으로 가장 잘 알려져 있습니다. 그는 1418년부터 1450년까지 통치하였으며, 그의 통치 기간 동안 과학, 기술, 문화, 농업 등 여러 분야에서 많은 발전이 있었습니다. 세종대왕은 백성을 사랑하고 그들의 삶을 개선하기 위해 노력한 왕으로 평가받고 있습니다. 그의 업적 중에는 천문학, 의학, 음악 등 다양한 분야에서의 기여가 포함되어 있습니다.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '세종대왕이 누구야',\n",
       " 'chat_history': '',\n",
       " 'output': '세종대왕(세종대왕, 1397-1450)은 조선의 제4대 왕으로, 한글을 창제한 것으로 가장 잘 알려져 있습니다. 그는 1418년부터 1450년까지 통치하였으며, 그의 통치 기간 동안 과학, 기술, 문화, 농업 등 여러 분야에서 많은 발전이 있었습니다. 세종대왕은 백성을 사랑하고 그들의 삶을 개선하기 위해 노력한 왕으로 평가받고 있습니다. 그의 업적 중에는 천문학, 의학, 음악 등 다양한 분야에서의 기여가 포함되어 있습니다.\\n```'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": '세종대왕이 누구야'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "699352fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "Final Answer: 한양대학교 데이터사이언스학과는 데이터 분석, 머신러닝, 인공지능, 빅데이터 처리 등 다양한 분야를 연구하고 교육하는 학부입니다. 이 학과는 데이터의 수집, 처리, 분석 및 활용에 대한 이론과 실습을 제공하며, 학생들이 데이터 기반의 문제 해결 능력을 기를 수 있도록 돕습니다. 또한, 산업계와의 협력을 통해 실무 경험을 쌓을 수 있는 기회를 제공하고 있습니다. 더 구체적인 정보는 한양대학교 공식 웹사이트나 데이터사이언스학과의 페이지를 통해 확인할 수 있습니다.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '한양대학교 데이터사이언스학과 정보 알려줘?',\n",
       " 'chat_history': 'Human: 세종대왕이 누구야\\nAI: 세종대왕(세종대왕, 1397-1450)은 조선의 제4대 왕으로, 한글을 창제한 것으로 가장 잘 알려져 있습니다. 그는 1418년부터 1450년까지 통치하였으며, 그의 통치 기간 동안 과학, 기술, 문화, 농업 등 여러 분야에서 많은 발전이 있었습니다. 세종대왕은 백성을 사랑하고 그들의 삶을 개선하기 위해 노력한 왕으로 평가받고 있습니다. 그의 업적 중에는 천문학, 의학, 음악 등 다양한 분야에서의 기여가 포함되어 있습니다.\\n```\\nHuman: 한양대학교 한경삭 교수님이 누구야??\\nAI: 한양대학교의 한경삭 교수님은 데이터사이언스학부에서 인간 중심 인공지능 및 데이터사이언스 분야를 전문으로 연구하고 가르치고 계십니다. 교수님의 연구는 인간-컴퓨터 상호작용과 관련된 주제들을 포함하고 있습니다. 추가적인 정보는 한양대학교의 공식 웹사이트에서 확인할 수 있습니다.\\nHuman: 한양대학교 한경식 교수님이 누구야??\\nAI: 한양대학교의 한경식 교수님은 데이터사이언스학부의 부교수이자 학부장으로, 인간-컴퓨터 상호작용 및 인간 중심 인공지능, 데이터사이언스 분야를 전문으로 연구하고 가르치고 계십니다. 교수님의 연구는 컴퓨터 기술과 인간 활동 간의 관계를 탐구하며, 사람에게 도움이 되는 기술 개발에 중점을 두고 있습니다. 연락처는 02-2220-2517이며, 이메일은 kyungsikhan@hanyang.ac.kr입니다.',\n",
       " 'output': '한양대학교 데이터사이언스학과는 데이터 분석, 머신러닝, 인공지능, 빅데이터 처리 등 다양한 분야를 연구하고 교육하는 학부입니다. 이 학과는 데이터의 수집, 처리, 분석 및 활용에 대한 이론과 실습을 제공하며, 학생들이 데이터 기반의 문제 해결 능력을 기를 수 있도록 돕습니다. 또한, 산업계와의 협력을 통해 실무 경험을 쌓을 수 있는 기회를 제공하고 있습니다. 더 구체적인 정보는 한양대학교 공식 웹사이트나 데이터사이언스학과의 페이지를 통해 확인할 수 있습니다.\\n```'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": '한양대학교 데이터사이언스학과 정보 알려줘?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f008c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
